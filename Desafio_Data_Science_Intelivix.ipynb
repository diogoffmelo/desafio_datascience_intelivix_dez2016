{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##imports\n",
    "\n",
    "import random as rd\n",
    "from pprint import pprint\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import sklearn \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##configuracao e funcoes auxiliares\"\"\"\n",
    "\n",
    "##percentagem do conjunto de instancias que sera separado para validacao do classificador\n",
    "test_set_rate=0.34\n",
    "\n",
    "\n",
    "##numero de folds usado na estimativa dos hiperparametros de cada modelo\n",
    "hyper_cv_folds = 3\n",
    "\n",
    "##Permite que os resultados sejam reproduzidos exatamente usando a mesma semente \n",
    "##aleatoria. Defina False para habilitar resultados aleatorios\n",
    "fixed_random_state = True\n",
    "\n",
    "if fixed_random_state:\n",
    "    rd.seed(1)\n",
    "else:\n",
    "    rd.seed()\n",
    "\n",
    "#wrap funcion para o gerador de sementes aleatorias\n",
    "def getRandIntState():\n",
    "    if fixed_random_state:\n",
    "        return 0\n",
    "    else:\n",
    "        return rd.getstate()\n",
    "\n",
    "##pre processador     \n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def analyzer1(words):\n",
    "    ret = []\n",
    "    \n",
    "    for w in words:\n",
    "        ##Remove stop words\n",
    "        if w in stopwords.words(\"english\"):\n",
    "            continue\n",
    "        ##Remove acentos, tranforma em minusculas\n",
    "        w = w.translate(translator).lower()\n",
    "        \n",
    "        ##remove palavras vazias\n",
    "        if len(w) == 0:\n",
    "            continue\n",
    "        \n",
    "        ret.append(stemmer.stem(w))\n",
    "    \n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de 155 instancias\n",
      "Sendo 80 da classe \"learned\" e 75 da classe \"belles_lettres\"\n",
      "Instancias: 102 treino e 53 teste\n",
      "Transformacao tdf-idf\n",
      "\n",
      "Exemplo de um texto de entrada:\n",
      "\n",
      "['1', '.', 'Introduction', 'It', 'has', 'recently', 'become', 'practical', 'to', 'use', 'the', 'radio', 'emission', 'of', 'the', 'moon', 'and', 'planets', 'as', 'a', 'new', 'source', 'of', 'information', 'about', 'these', 'bodies', 'and', 'their', 'atmospheres', '.', 'The', 'results', 'of', 'present', 'observations', 'of', 'the', 'thermal', 'radio', 'emission', 'of', 'the', 'moon', 'are', 'consistent', 'with', 'the', 'very', 'low', 'thermal', 'conductivity', 'of', 'the', 'surface', 'layer', 'which', 'was', 'derived', 'from', 'the', 'variation', 'in', 'the', 'infrared', 'emission', 'during', 'eclipses', '(', 'e.g.', ',', 'Garstung', ',', '1958', ')', '.', 'When', 'sufficiently', 'accurate', 'and', 'complete', 'measurements', 'are', 'available', ',', 'it', 'will', 'be', 'possible', 'to', 'set', 'limits', 'on', 'the', 'thermal', 'and', 'electrical', 'characteristics', 'of', 'the']...\n",
      "\n",
      "Exemplo apos analyzer:\n",
      "\n",
      "['1', 'introduct', 'it', 'recent', 'becom', 'practic', 'use', 'radio', 'emiss', 'moon', 'planet', 'new', 'sourc', 'inform', 'bodi', 'atmospher', 'the', 'result', 'present', 'observ', 'thermal', 'radio', 'emiss', 'moon', 'consist', 'low', 'thermal', 'conduct', 'surfac', 'layer', 'deriv', 'variat', 'infrar', 'emiss', 'eclips', 'eg', 'garstung', '1958', 'when', 'suffici', 'accur', 'complet', 'measur', 'avail', 'possibl', 'set', 'limit', 'thermal', 'electr', 'characterist']...\n",
      "\n",
      "Exemplo apos feature extraction:\n",
      "\n",
      "\n",
      "1\t\t[ 0.06860397]\n",
      "introduct\t\t[ 0.12900322]\n",
      "it\t\t[ 0.04016942]\n",
      "recent\t\t[ 0.07397429]\n",
      "becom\t\t[ 0.05780779]\n",
      "practic\t\t[ 0.0749491]\n",
      "use\t\t[ 0.04563005]\n",
      "radio\t\t[ 0.2668468]\n",
      "emiss\t\t[ 0.55634543]\n",
      "moon\t\t[ 0.28843916]\n",
      "planet\t\t[ 0.15106177]\n",
      "new\t\t[ 0.0509656]\n",
      "sourc\t\t[ 0.0850926]\n",
      "inform\t\t[ 0.07397429]\n",
      "bodi\t\t[ 0.08919065]\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "##Obtencao das instancias, dos labels, divisao treino/test\n",
    "\n",
    "##cria uma distribuicao de frequencias para as categoria no corpo\n",
    "freq_dist = {cat:len(brown.fileids(cat)) for cat in brown.categories()}\n",
    "\n",
    "##Seleciona as duas categorias mais comuns\n",
    "more_freq = sorted(freq_dist, key=lambda x: -freq_dist[x])[0:2]\n",
    "\n",
    "##Extrai os ids das duas categorias mais comuns e cria os labels\n",
    "raw_set = [(brown.words(text_id), more_freq[0]) for text_id in brown.fileids(more_freq[0])] + \\\n",
    "    [(brown.words(text_id), more_freq[1]) for text_id in brown.fileids(more_freq[1])]\n",
    "\n",
    "#raw_set = [(brown.raw(text_id), more_freq[0]) for text_id in brown.fileids(more_freq[0])] + \\\n",
    "#    [(brown.raw(text_id), more_freq[1]) for text_id in brown.fileids(more_freq[1])]\n",
    "\n",
    "\n",
    "print (\"Total de {} instancias\".format(len(raw_set)))    \n",
    "print (\"Sendo {} da classe \\\"{}\\\" e {} da classe \\\"{}\\\"\".format(\n",
    "                                                                len(brown.fileids(more_freq[0])),\n",
    "                                                                more_freq[0], \n",
    "                                                                len(brown.fileids(more_freq[1])),\n",
    "                                                                more_freq[1]))\n",
    "\n",
    "\n",
    "##Conjunto das instancias\n",
    "X = [r[0] for r in raw_set]\n",
    "\n",
    "##Conjunto de labels\n",
    "y = [r[1] for r in raw_set]\n",
    "\n",
    "##Divide os conjuntos em teste e treino\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_set_rate, \n",
    "                                                    stratify=y, random_state=getRandIntState())\n",
    "\n",
    "\n",
    "print (\"Instancias: {} treino e {} teste\".format(len(y_train), len(y_test)))    \n",
    "    \n",
    "    \n",
    "##O fit e feito apenas no conjunto de treino para evitar information leak \n",
    "##para a etapa de validacao\n",
    "tfidfBuilder = TfidfVectorizer(X_train, analyzer=analyzer1)\n",
    "\n",
    "##Extrai os features para o conjunto de testes e de treino\n",
    "X_train = tfidfBuilder.fit_transform(X_train)\n",
    "X_test = tfidfBuilder.transform(X_test)\n",
    "\n",
    "print (\"Transformacao tdf-idf\")\n",
    "ex_raw = raw_set[0][0][:min(100, len(raw_set[0][0]))]\n",
    "ex_tkn = list(tfidfBuilder.build_analyzer()(ex_raw))\n",
    "ex_trans = tfidfBuilder.transform([ex_raw])\n",
    "\n",
    "print (\"\\nExemplo de um texto de entrada:\\n\\n{}...\".format(ex_raw))\n",
    "print (\"\\nExemplo apos analyzer:\\n\\n{}...\".format(ex_tkn))\n",
    "print (\"\\nExemplo apos feature extraction:\\n\\n\")\n",
    "       \n",
    "for vocab in ex_tkn[:15]:\n",
    "    if vocab in tfidfBuilder.vocabulary_:\n",
    "        fidf = ex_trans.getcol(tfidfBuilder.vocabulary_[vocab]).data\n",
    "        print (\"{}\\t\\t{}\".format(vocab, fidf))\n",
    "\n",
    "print(\"...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning dos hiper-parametros\n",
      "Decision Tree:\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param set 0\n",
      "{'criterion': 'entropy', 'max_depth': 4, 'max_features': None}\n",
      "scr:0.7156862745098039+/-0.031123277592579726. rnk1\n",
      "param set 1\n",
      "{'criterion': 'entropy', 'max_depth': 4, 'max_features': 'sqrt'}\n",
      "scr:0.5980392156862745+/-0.2935738968274183. rnk20\n",
      "param set 2\n",
      "{'criterion': 'entropy', 'max_depth': 4, 'max_features': 'log2'}\n",
      "scr:0.6568627450980392+/-0.14005296660786556. rnk6\n",
      "param set 3\n",
      "{'criterion': 'entropy', 'max_depth': 8, 'max_features': None}\n",
      "scr:0.7156862745098039+/-0.031123277592579726. rnk1\n",
      "param set 4\n",
      "{'criterion': 'entropy', 'max_depth': 8, 'max_features': 'sqrt'}\n",
      "scr:0.5882352941176471+/-0.2204646967537368. rnk22\n",
      "param set 5\n",
      "{'criterion': 'entropy', 'max_depth': 8, 'max_features': 'log2'}\n",
      "scr:0.6274509803921569+/-0.09663883998904749. rnk14\n",
      "param set 6\n",
      "{'criterion': 'entropy', 'max_depth': 16, 'max_features': None}\n",
      "scr:0.7156862745098039+/-0.031123277592579726. rnk1\n",
      "param set 7\n",
      "{'criterion': 'entropy', 'max_depth': 16, 'max_features': 'sqrt'}\n",
      "scr:0.5882352941176471+/-0.2204646967537368. rnk22\n",
      "param set 8\n",
      "{'criterion': 'entropy', 'max_depth': 16, 'max_features': 'log2'}\n",
      "scr:0.6470588235294118+/-0.04231453804096733. rnk11\n",
      "param set 9\n",
      "{'criterion': 'entropy', 'max_depth': None, 'max_features': None}\n",
      "scr:0.7156862745098039+/-0.031123277592579726. rnk1\n",
      "param set 10\n",
      "{'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt'}\n",
      "scr:0.5882352941176471+/-0.2204646967537368. rnk22\n",
      "param set 11\n",
      "{'criterion': 'entropy', 'max_depth': None, 'max_features': 'log2'}\n",
      "scr:0.5980392156862745+/-0.1804104241146725. rnk20\n",
      "param set 12\n",
      "{'criterion': 'gini', 'max_depth': 4, 'max_features': None}\n",
      "scr:0.6568627450980392+/-0.1676992831718037. rnk6\n",
      "param set 13\n",
      "{'criterion': 'gini', 'max_depth': 4, 'max_features': 'sqrt'}\n",
      "scr:0.6666666666666666+/-0.23391711492520792. rnk5\n",
      "param set 14\n",
      "{'criterion': 'gini', 'max_depth': 4, 'max_features': 'log2'}\n",
      "scr:0.6372549019607843+/-0.16982948435283127. rnk12\n",
      "param set 15\n",
      "{'criterion': 'gini', 'max_depth': 8, 'max_features': None}\n",
      "scr:0.6568627450980392+/-0.1676992831718037. rnk6\n",
      "param set 16\n",
      "{'criterion': 'gini', 'max_depth': 8, 'max_features': 'sqrt'}\n",
      "scr:0.6274509803921569+/-0.16664194906512686. rnk14\n",
      "param set 17\n",
      "{'criterion': 'gini', 'max_depth': 8, 'max_features': 'log2'}\n",
      "scr:0.6372549019607843+/-0.11558329073236027. rnk12\n",
      "param set 18\n",
      "{'criterion': 'gini', 'max_depth': 16, 'max_features': None}\n",
      "scr:0.6568627450980392+/-0.1676992831718037. rnk6\n",
      "param set 19\n",
      "{'criterion': 'gini', 'max_depth': 16, 'max_features': 'sqrt'}\n",
      "scr:0.6274509803921569+/-0.16664194906512686. rnk14\n",
      "param set 20\n",
      "{'criterion': 'gini', 'max_depth': 16, 'max_features': 'log2'}\n",
      "scr:0.6078431372549019+/-0.11519384347316412. rnk18\n",
      "param set 21\n",
      "{'criterion': 'gini', 'max_depth': None, 'max_features': None}\n",
      "scr:0.6568627450980392+/-0.1676992831718037. rnk6\n",
      "param set 22\n",
      "{'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt'}\n",
      "scr:0.6274509803921569+/-0.16664194906512686. rnk14\n",
      "param set 23\n",
      "{'criterion': 'gini', 'max_depth': None, 'max_features': 'log2'}\n",
      "scr:0.6078431372549019+/-0.11519384347316412. rnk18\n",
      "SVM-Classifier:\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed:   21.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param set 0\n",
      "{'C': 0.0001, 'kernel': 'linear'}\n",
      "scr:0.5196078431372549+/-0.013882833453779676. rnk6\n",
      "param set 1\n",
      "{'C': 0.1, 'kernel': 'linear'}\n",
      "scr:0.5196078431372549+/-0.013882833453779676. rnk6\n",
      "param set 2\n",
      "{'C': 10, 'kernel': 'linear'}\n",
      "scr:0.8529411764705882+/-0.09336983277773916. rnk1\n",
      "param set 3\n",
      "{'C': 100, 'kernel': 'linear'}\n",
      "scr:0.8529411764705882+/-0.09336983277773916. rnk1\n",
      "param set 4\n",
      "{'C': 0.0001, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "scr:0.5196078431372549+/-0.013882833453779676. rnk6\n",
      "param set 5\n",
      "{'C': 0.0001, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "scr:0.5196078431372549+/-0.013882833453779676. rnk6\n",
      "param set 6\n",
      "{'C': 0.0001, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "scr:0.5196078431372549+/-0.013882833453779676. rnk6\n",
      "param set 7\n",
      "{'C': 0.0001, 'gamma': 10, 'kernel': 'rbf'}\n",
      "scr:0.5196078431372549+/-0.013882833453779676. rnk6\n",
      "param set 8\n",
      "{'C': 0.0001, 'gamma': 100, 'kernel': 'rbf'}\n",
      "scr:0.5196078431372549+/-0.013882833453779676. rnk6\n",
      "param set 9\n",
      "{'C': 0.1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "scr:0.5196078431372549+/-0.013882833453779676. rnk6\n",
      "param set 10\n",
      "{'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "scr:0.5196078431372549+/-0.013882833453779676. rnk6\n",
      "param set 11\n",
      "{'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "scr:0.5196078431372549+/-0.013882833453779676. rnk6\n",
      "param set 12\n",
      "{'C': 0.1, 'gamma': 10, 'kernel': 'rbf'}\n",
      "scr:0.5196078431372549+/-0.013882833453779676. rnk6\n",
      "param set 13\n",
      "{'C': 0.1, 'gamma': 100, 'kernel': 'rbf'}\n",
      "scr:0.5196078431372549+/-0.013882833453779676. rnk6\n",
      "param set 14\n",
      "{'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "scr:0.5196078431372549+/-0.013882833453779676. rnk6\n",
      "param set 15\n",
      "{'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "scr:0.5196078431372549+/-0.013882833453779676. rnk6\n",
      "param set 16\n",
      "{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "scr:0.8529411764705882+/-0.09336983277773916. rnk1\n",
      "param set 17\n",
      "{'C': 10, 'gamma': 10, 'kernel': 'rbf'}\n",
      "scr:0.5196078431372549+/-0.013882833453779676. rnk6\n",
      "param set 18\n",
      "{'C': 10, 'gamma': 100, 'kernel': 'rbf'}\n",
      "scr:0.5196078431372549+/-0.013882833453779676. rnk6\n",
      "param set 19\n",
      "{'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "scr:0.5196078431372549+/-0.013882833453779676. rnk6\n",
      "param set 20\n",
      "{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "scr:0.8529411764705882+/-0.09336983277773916. rnk1\n",
      "param set 21\n",
      "{'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "scr:0.8529411764705882+/-0.09336983277773916. rnk1\n",
      "param set 22\n",
      "{'C': 100, 'gamma': 10, 'kernel': 'rbf'}\n",
      "scr:0.5196078431372549+/-0.013882833453779676. rnk6\n",
      "param set 23\n",
      "{'C': 100, 'gamma': 100, 'kernel': 'rbf'}\n",
      "scr:0.5196078431372549+/-0.013882833453779676. rnk6\n",
      "Multilayer perceptron classifier:\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:   44.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param set 0\n",
      "{'hidden_layer_sizes': (5, 5)}\n",
      "scr:0.803921568627451+/-0.1918489605320673. rnk2\n",
      "param set 1\n",
      "{'hidden_layer_sizes': (15, 15)}\n",
      "scr:0.7745098039215687+/-0.1103998163701165. rnk4\n",
      "param set 2\n",
      "{'hidden_layer_sizes': (10, 5)}\n",
      "scr:0.803921568627451+/-0.07919162979628427. rnk2\n",
      "param set 3\n",
      "{'hidden_layer_sizes': (15, 15)}\n",
      "scr:0.7745098039215687+/-0.1103998163701165. rnk4\n",
      "param set 4\n",
      "{'hidden_layer_sizes': (20, 50)}\n",
      "scr:0.8333333333333334+/-0.144985813350097. rnk1\n",
      "param set 5\n",
      "{'hidden_layer_sizes': (50, 20)}\n",
      "scr:0.7647058823529411+/-0.07422512730342977. rnk6\n",
      "fim do treino\n"
     ]
    }
   ],
   "source": [
    "##Busca dos parametros\n",
    "\n",
    "    \n",
    "def selectBestParametersSet (clf, parameters):\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "                                estimator=clf, \n",
    "                                param_grid=parameters, \n",
    "                                cv=StratifiedKFold(\n",
    "                                                    n_splits=hyper_cv_folds, \n",
    "                                                    random_state=getRandIntState()),\n",
    "                                verbose=1,\n",
    "                                scoring=\"accuracy\")\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    results = grid_search.cv_results_\n",
    "    params = results['params']\n",
    "    \n",
    "    for t in range(0, len(params)):\n",
    "        print (\"param set {}\".format(t))\n",
    "        pprint(params[t])\n",
    "\n",
    "        mean = results['mean_test_score'][t]\n",
    "        std = 2*results['std_test_score' ][t]\n",
    "        rkn = results['rank_test_score'][t]\n",
    "        \n",
    "        print(\"scr:{}+/-{}. rnk{}\".format(mean, std, rkn))\n",
    "\n",
    "        \n",
    "    best_clf = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_ \n",
    "\n",
    "    best_clf.fit(X_train, y_train)\n",
    "    \n",
    "    return {'clf':best_clf, 'params':best_params, 'pred':best_clf.predict(X_test)}\n",
    "\n",
    "print(\"Tuning dos hiper-parametros\")\n",
    "\n",
    "\n",
    "\n",
    "print (\"Decision Tree:\")\n",
    "\n",
    "tree = selectBestParametersSet(\n",
    "                                DecisionTreeClassifier(random_state=getRandIntState()),\n",
    "                               {'max_depth':[4, 8, 16, None], \n",
    "                                'criterion' : ['entropy', 'gini'],\n",
    "                                'max_features' : [None, 'sqrt', 'log2']\n",
    "                               })\n",
    "\n",
    "\n",
    "\n",
    "print (\"SVM-Classifier:\")\n",
    "\n",
    "svm = selectBestParametersSet(\n",
    "                                SVC(random_state=getRandIntState()),\n",
    "                                [{'kernel':['linear'], 'C':[0.0001, 0.1, 10, 100]} , \n",
    "                                 { 'kernel':['rbf'], 'C':[0.0001, 0.1, 10, 100], \n",
    "                                  'gamma' : [0.0001, 0.01, 0.1, 10, 100]}]\n",
    "                               )\n",
    "\n",
    "print (\"Multilayer perceptron classifier:\")\n",
    "\n",
    "mlpc = selectBestParametersSet(\n",
    "                                MLPClassifier(\n",
    "                                    random_state=getRandIntState(), \n",
    "                                    max_iter=2000, \n",
    "                                    solver='lbfgs'),\n",
    "                               {'hidden_layer_sizes':[(5, 5), (15,15), (10,5), (15, 15), \n",
    "                                                      (20, 50), (50,20)]})\n",
    "\n",
    "\n",
    "\n",
    "print(\"fim do treino\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================\n",
      "Arvore de Decisao\n",
      "Melhores parametros => {'criterion': 'entropy', 'max_features': None, 'max_depth': 4}\n",
      "\n",
      "Matriz de confusao:\n",
      "[[21  5]\n",
      " [ 6 21]]\n",
      "\n",
      "Coeficiente de correlacao de Matthews:\n",
      "0.58547008547\n",
      "\n",
      "Acuracia e precisao:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "belles_lettres       0.78      0.81      0.79        26\n",
      "       learned       0.81      0.78      0.79        27\n",
      "\n",
      "   avg / total       0.79      0.79      0.79        53\n",
      "\n",
      "================================================================================\n",
      "=================================================================================\n",
      "SVM\n",
      "Melhores parametros => {'kernel': 'linear', 'C': 10}\n",
      "\n",
      "Matriz de confusao:\n",
      "[[24  2]\n",
      " [ 6 21]]\n",
      "\n",
      "Coeficiente de correlacao de Matthews:\n",
      "0.706922820153\n",
      "\n",
      "Acuracia e precisao:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "belles_lettres       0.80      0.92      0.86        26\n",
      "       learned       0.91      0.78      0.84        27\n",
      "\n",
      "   avg / total       0.86      0.85      0.85        53\n",
      "\n",
      "================================================================================\n",
      "=================================================================================\n",
      "MLPC\n",
      "Melhores parametros => {'hidden_layer_sizes': (20, 50)}\n",
      "\n",
      "Matriz de confusao:\n",
      "[[24  2]\n",
      " [ 6 21]]\n",
      "\n",
      "Coeficiente de correlacao de Matthews:\n",
      "0.706922820153\n",
      "\n",
      "Acuracia e precisao:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "belles_lettres       0.80      0.92      0.86        26\n",
      "       learned       0.91      0.78      0.84        27\n",
      "\n",
      "   avg / total       0.86      0.85      0.85        53\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def show_results(name, clf):\n",
    "    print (\"=================================================================================\")\n",
    "    print (name)\n",
    "    print (\"Melhores parametros => {}\\n\".format(clf['params']))\n",
    "    print (\"Matriz de confusao:\")\n",
    "    print (confusion_matrix(y_test, clf['pred']))\n",
    "    print (\"\\nCoeficiente de correlacao de Matthews:\")\n",
    "    print (matthews_corrcoef(y_test, clf['pred']))\n",
    "    print (\"\\nAcuracia e precisao:\")\n",
    "    print (classification_report(y_test, clf['pred']))\n",
    "    print (\"================================================================================\")\n",
    "\n",
    "show_results(\"Arvore de Decisao\", tree)\n",
    "show_results(\"SVM\", svm)\n",
    "show_results(\"MLPC\", mlpc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Desafio Data Science Intelivix - Avancado__\n",
    "\n",
    "\n",
    "O corpus foi importado e a populacao de cada genero calculada, sendo os generos 'belles_lettres' e 'learned' os mais populosos. As entradas foram divididas em um conjunto de treino e um conjunto de testes. Cada entrada foi entao tokenizada utilizando-se a funcao analyzer1. Os tokens foram transformados em features utlizando-se a funcao TfidfVectorizer.\n",
    "\n",
    "Tres classificadores foram escolhidos por representarem distintas metodologias de classificacao: \n",
    "\n",
    "1. Arvores de decisao, que a cada passo escolhe que caracteristica melhor divide as instancias baseado na minimizacao de algum criterio, isto e, qual o feature mais representativo para classificar uma instancia.\n",
    "\n",
    "2. Maquinas de Vetores de suporte (SVM), que encontra quais as instancias mais representativas para se criar uma regiao de fronteira entre as duas classes das instancias.\n",
    "\n",
    "3. Multilayer Perceptron (MLPC), que tenta criar uma representacao abstrata para as instancias investigando como combinacoes (somas) dos features podem decidir a classe que uma instacia pertence.\n",
    "\n",
    "Cada um dos classificadores apresenta parametros que devem ser \"tunados\". Para cada classificador foram escolhidos os parametros mais representativos e um esquema 3-fold com estratificacao do conjunto de treino foi usado para definir o melhor conjunto de valores para esses parametros. A acuracia media (entre os folds) para cada conjunto de valores dos parametros foi utilizada como estimador para rankear e definir o melhor conjunto de valores para cada classificador.\n",
    "\n",
    "Cada classificador tunado com os parametros otimos da etapa anterior foi treinado com todo o conjunto de treino e validado sob o conjunto de testes. Segue a analise dos resultados:\n",
    "\n",
    "1. Arvores de decisao: Foi o classificador mais rapido a ser treinado, levando apenas 3.9s em todas as 72 dombinacoes de valores/folds. Em contrapartida, foi o que apresentou mais erros de classificacao, o que pode ser notado em pelo valor da acuracia e do coeficiente de correlacao de Matthews (CCW). Apresentou uma sensibilidade moderada a escolha dos parametros, apresentando uma diferenca de 0.127 entre sua melhor e pior acuracia. Alem disso, apresentou e uma alta variacao entre os folds (note o erro na estimativa da acuracia entre os folds), chagando a apresentar desvios da de 0.3 para alguns valores dos parametros. Altos valores do desvio padrao entre os folds sugerem uma dependencia do modelo com o conjunto de treino.\n",
    "\n",
    "2. SVM: Foi o segundo classificador mais rapido, demandando 21.6s para ser treinado em todas as 72 possiveis combinacoes. Apresentou uma acuracia e um CCW superiores aos da arvore e decisao, estado empatado com o MLPC. Apresentou a maior sensibilidade aos parametros, com uma diferenca na acuracia de 0.334 entre o melhor e o pior caso. Entretanto, apresentou a menor variancia (maior estabilidade) dentre todos os classificadores entre os folds (independencia do conjunto de treino), sendo o maior desvio de 0.01. Este comprotamento sugere que uma invertigacao mais fina dos parametros pode levar a uma melhora desempenho deste classificador sem afetar sua estabilidade.\n",
    "\n",
    "\n",
    "3. MLPC: Empatado com o SVM, foi o classificador mais lento, levando 44.0s para ser treinado nas 18 combinacoes possiveis. Apresentou a menor variacao com os parametros, sendo a diferenca entre o melhor e o pior caso da acuracia de 0.070. Entretanto, apresentou instabilidade entre as execucoes em diferentes foldes com os valores de paramentros fixados, mostrando desvios da ordem de 0.2. Isso sugere que poucas melhorias podem ser obtidas pelo ajuste dos parametros desse classificar. Seu tempo elevado de treinamento mostra-se um ponto negativo dessa tecnica.\n",
    "\n",
    "\n",
    "Em resumo, a arvode de decisao mostrou-se um classificador rapido no treinamento porem, instavel e de acuracia menor. O MLPC mostra uma uma acuracia robusta para diferentes parametros, porem e lento e apresenta uma certa dependencia no conjunto de treino. O SVM foi o melhor classificador, com uma acuracia similar ao MLPC, um tempo de treinamento intermediario, menor dependencia com o conjunto de treino e possibilidades de melhorias. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
